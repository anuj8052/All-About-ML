Here is a list of classical machine learning models, categorized by their types:

### 1. **Linear Models**
   - **Linear Regression**: Used for predicting continuous values.
   - **Logistic Regression**: A classification algorithm that predicts the probability of a binary outcome.
   - **Ridge Regression**: Linear regression with L2 regularization.
   - **Lasso Regression**: Linear regression with L1 regularization.
   - **Elastic Net**: Combines L1 and L2 regularization.

### 2. **Tree-Based Models**
   - **Decision Tree**: Classification and regression model that splits data based on feature values.
   - **Random Forest**: An ensemble of decision trees used for classification and regression.
   - **Gradient Boosting Machine (GBM)**: Builds sequential trees with the goal of minimizing the residual error.
   - **XGBoost**: An optimized implementation of gradient boosting with performance improvements.
   - **LightGBM**: A gradient boosting framework that uses a leaf-wise tree growth algorithm.
   - **CatBoost**: Gradient boosting on decision trees that is especially effective with categorical features.

### 3. **Support Vector Machines (SVM)**
   - **Support Vector Classifier (SVC)**: Classification using support vector machines, optimized to maximize the margin between classes.
   - **Support Vector Regressor (SVR)**: Regression using support vector machines.

### 4. **Naive Bayes Models**
   - **Gaussian Naive Bayes**: Assumes features are normally distributed.
   - **Multinomial Naive Bayes**: Useful for discrete data, such as text classification.
   - **Bernoulli Naive Bayes**: Assumes binary features.

### 5. **k-Nearest Neighbors (k-NN)**
   - **k-NN for Classification**: Classifies data points based on the majority class among the k nearest neighbors.
   - **k-NN for Regression**: Predicts a value based on the average of the k nearest neighbors.

### 6. **Clustering Models**
   - **k-Means Clustering**: Clustering algorithm that groups data points based on feature similarity.
   - **Hierarchical Clustering**: Clusters data in a tree-like structure.
   - **DBSCAN**: Density-based clustering that can find clusters of arbitrary shapes.

### 7. **Dimensionality Reduction Models**
   - **Principal Component Analysis (PCA)**: Reduces dimensionality by transforming data to principal components.
   - **Linear Discriminant Analysis (LDA)**: Reduces dimensionality while preserving class-discriminative information.
   - **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: Reduces dimensionality while preserving the local structure.

### 8. **Neural Networks (for classical ML applications)**
   - **Perceptron**: The simplest type of artificial neuron for binary classification.
   - **Multi-Layer Perceptron (MLP)**: Feedforward neural network with one or more hidden layers, used for classification and regression tasks.

### 9. **Ensemble Models**
   - **Bagging** (e.g., Bagged Trees): Combines multiple models trained independently.
   - **AdaBoost**: Sequentially adds weak learners, focusing on instances where errors were made.
   - **Stacking**: Combines predictions from multiple models using another model to learn from their outputs.

### 10. **Anomaly Detection Models**
   - **Isolation Forest**: Identifies anomalies by isolating points using random partitions.
   - **One-Class SVM**: A version of SVM designed for outlier detection.

These models are widely used and form the foundation of classical machine learning. Each of them has variations or optimizations that can be tailored to specific tasks.
