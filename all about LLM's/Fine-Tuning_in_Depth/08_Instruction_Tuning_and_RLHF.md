# Module 8: Instruction Tuning and RLHF

## 1. Beyond Predicting Text: The Goal of Alignment

Base language models are trained to predict the next word in a sequence. While powerful, this doesn't automatically make them helpful assistants. If you give a base model the prompt "Write a function in Python to get the current date," it might simply continue with "and time, and format it as a string." It doesn't inherently understand the *intent* to act as a helpful agent.

The goal of **alignment** is to steer the model's behavior to be helpful, honest, and harmless. This is primarily achieved through two powerful techniques: Instruction Tuning and Reinforcement Learning from Human Feedback (RLHF).

## 2. Instruction Tuning (Supervised Fine-Tuning - SFT)

Instruction tuning is the first and most fundamental step in alignment. It teaches the model how to follow instructions and act like a conversational AI.

### The Process:

1.  **Start with a Base Model:** Take a powerful, pre-trained base LLM (e.g., Llama-2, Mistral).
2.  **Create a High-Quality Dataset:** Curate or create a dataset of thousands of high-quality examples. Each example consists of an **instruction** (a prompt or question) and a high-quality **response**.
    ```json
    {
      "instruction": "Explain the theory of relativity in simple terms.",
      "response": "Imagine you're on a train... Albert Einstein's theory of relativity has two parts..."
    }
    ```
3.  **Fine-Tune:** Fine-tune the base model on this dataset using standard supervised learning. The model's task is to learn to generate the `response` when given the `instruction`. This fine-tuning can be done using either Full Fine-Tuning or, more commonly, a PEFT method like LoRA/QLoRA.

The result of this stage is a **Supervised Fine-Tuned (SFT) model**. It is now much better at following instructions and behaving in a conversational manner.

## 3. The Limits of SFT and the Need for RLHF

SFT is great for teaching the model the *format* of a good answer, but it has limitations. It's very difficult and expensive to create a perfect dataset that captures all the nuances of what makes a response good, safe, and helpful. How do you teach a model to be creative but not make things up? How do you teach it to refuse harmful requests politely?

This is where RLHF comes in. It allows us to teach the model about **preferences**—what makes one response *better* than another—which is a more scalable and effective way to instill nuanced human values.

## 4. Reinforcement Learning from Human Feedback (RLHF)

RLHF is a complex, three-stage process that uses reinforcement learning to align an LLM with human preferences.

### Stage 1: Supervised Fine-Tuning (SFT)

This is exactly the process described above. We start with an SFT model because we need a model that already understands how to follow instructions reasonably well.

### Stage 2: Reward Model Training

This is the heart of RLHF. The goal is to create a model that can **score a response based on how much a human would like it**.

1.  **Collect Preference Data:**
    -   Take a variety of prompts (instructions).
    -   For each prompt, use the SFT model to generate several different responses (e.g., 4 or 5).
    -   Present these responses to human labelers and ask them to **rank them from best to worst**. (e.g., Response D > Response B > Response A > Response C).
2.  **Train a Reward Model (RM):**
    -   The reward model is typically another LLM (often initialized from the SFT model or a smaller variant).
    -   It is trained on the preference data. Its goal is to take a prompt and a response as input and output a single scalar value (a "reward score").
    -   The RM is trained to assign a higher score to the human-preferred response in a pair. For example, for the pair (Response D, Response B), the RM is trained to give a higher score to D.

At the end of this stage, we have a model that can act as a proxy for human judgment.

### Stage 3: RL Fine-Tuning with PPO

In this final stage, we use the reward model to fine-tune our SFT model.

1.  **The RL Setup:**
    -   The **agent** is our SFT model.
    -   The **environment** is the space of all possible prompts.
    -   The **action** is the response generated by the SFT model.
    -   The **reward** is the score given by our trained reward model.
2.  **The PPO Algorithm:**
    -   A prompt is sampled from the dataset and fed to the SFT model, which generates a response.
    -   The response is shown to the **reward model**, which calculates a reward score.
    -   The **Proximal Policy Optimization (PPO)** algorithm uses this reward to calculate a policy gradient and update the weights of the SFT model.
    -   **Crucial Constraint:** PPO includes a penalty term (a KL-divergence penalty) that prevents the model from changing too much from its original SFT version. This is critical to prevent the model from learning to generate gibberish that "hacks" the reward model to get a high score. It keeps the model coherent.

The model is iteratively updated to maximize the reward it receives from the reward model, effectively learning to generate responses that are highly aligned with human preferences. The final output of this entire process is a powerful, aligned, instruction-following LLM like ChatGPT or Claude.
